import streamlit as st
import requests
import json
from typing import Dict, List, Optional
import logging
from config import Config
import openai
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class QwenIntegration:
    def __init__(self):
        """Initialise l'int√©gration Qwen LLM"""
        self.config = Config()
        self.model = None
        self.tokenizer = None
        self.client = None
        self.hf_api_key = None
        self.mode = self._initialize_model()
    
    def _initialize_model(self) -> Optional[str]:
        """Initialise le mod√®le Qwen"""
        try:
            # Priorit√© 1: Hugging Face API (recommand√©)
            if self.config.HUGGINGFACE_API_KEY:
                self.hf_api_key = self.config.HUGGINGFACE_API_KEY
                logging.info("Initialisation avec Hugging Face API")
                return "huggingface_api"
            
            # Priorit√© 2: OpenAI API compatible
            elif self.config.OPENAI_API_KEY:
                self.client = openai.OpenAI(api_key=self.config.OPENAI_API_KEY)
                logging.info("Initialisation avec OpenAI API")
                return "openai_api"
            
            # Priorit√© 3: Mod√®le local (n√©cessite beaucoup de ressources)
            else:
                logging.info("Tentative de chargement du mod√®le local...")
                logging.warning("‚ö†Ô∏è Le chargement local n√©cessite beaucoup de RAM (>16GB)")
                
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.config.QWEN_MODEL,
                    trust_remote_code=True
                )
                
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.config.QWEN_MODEL,
                    device_map="auto",
                    trust_remote_code=True,
                    torch_dtype=torch.float16
                )
                
                logging.info("Mod√®le local Qwen charg√© avec succ√®s")
                return "local"
            
        except Exception as e:
            logging.error(f"Erreur lors du chargement du mod√®le: {str(e)}")
            return None
    
    def generate_response(self, prompt: str, max_tokens: int = 500, temperature: float = 0.7) -> str:
        """
        G√©n√®re une r√©ponse avec Qwen
        
        Args:
            prompt: Prompt d'entr√©e
            max_tokens: Nombre maximum de tokens
            temperature: Temp√©rature de g√©n√©ration
            
        Returns:
            R√©ponse g√©n√©r√©e
        """
        try:
            # M√©thode 1: Hugging Face Inference API
            if self.hf_api_key:
                # Try using a simpler, faster model first
                api_url = "https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2"
                headers = {
                    "Authorization": f"Bearer {self.hf_api_key}",
                    "Content-Type": "application/json"
                }
                
                # Format prompt for instruction-following
                formatted_prompt = f"[INST] {prompt} [/INST]"
                
                payload = {
                    "inputs": formatted_prompt,
                    "parameters": {
                        "max_new_tokens": max_tokens,
                        "temperature": temperature,
                        "return_full_text": False,
                        "do_sample": True,
                        "top_p": 0.9
                    },
                    "options": {
                        "wait_for_model": True
                    }
                }
                
                response = requests.post(api_url, headers=headers, json=payload, timeout=120)
                
                if response.status_code == 200:
                    result = response.json()
                    if isinstance(result, list) and len(result) > 0:
                        generated = result[0].get("generated_text", "")
                        return generated.strip()
                    elif isinstance(result, dict):
                        return result.get("generated_text", str(result)).strip()
                    return str(result).strip()
                    
                elif response.status_code == 503:
                    # Model is loading - wait and retry
                    logging.info("Model is loading, waiting 20 seconds...")
                    import time
                    time.sleep(20)
                    
                    # Retry once
                    response = requests.post(api_url, headers=headers, json=payload, timeout=120)
                    if response.status_code == 200:
                        result = response.json()
                        if isinstance(result, list) and len(result) > 0:
                            return result[0].get("generated_text", "").strip()
                    
                    logging.warning("Model still loading, falling back to basic mode")
                    return None
                    
                else:
                    logging.error(f"Erreur API HuggingFace: {response.status_code} - {response.text}")
                    return None
            
            # M√©thode 2: OpenAI API
            elif self.client:
                response = self.client.chat.completions.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=temperature
                )
                return response.choices[0].message.content
            
            # M√©thode 3: Mod√®le local
            elif self.model and self.tokenizer:
                inputs = self.tokenizer(prompt, return_tensors="pt")
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs.input_ids,
                        max_new_tokens=max_tokens,
                        temperature=temperature,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                return response[len(prompt):].strip()
            
            else:
                return None
                
        except requests.exceptions.Timeout:
            logging.warning("Request timeout - model may be loading or busy")
            return None
        except Exception as e:
            logging.error(f"Erreur g√©n√©ration: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def translate_text(self, text: str, source_lang: str, target_lang: str) -> str:
        """
        Traduit un texte de mani√®re contextuelle
        
        Args:
            text: Texte √† traduire
            source_lang: Langue source
            target_lang: Langue cible
            
        Returns:
            Texte traduit
        """
        if not text.strip():
            return ""
        
        # Essayer avec le mod√®le AI
        prompt = self.config.TRANSLATION_PROMPT.format(
            text=text,
            source_lang=source_lang,
            target_lang=target_lang
        )
        
        result = self.generate_response(prompt, max_tokens=300, temperature=0.3)
        
        if result is None:
            # Fallback: utiliser une traduction basique avec deep_translator
            try:
                from deep_translator import GoogleTranslator
                translator = GoogleTranslator(source='auto', target=target_lang)
                return translator.translate(text)
            except:
                return f"‚ö†Ô∏è Traduction non disponible. Veuillez configurer une cl√© API OpenAI dans le fichier .env\n\nTexte original: {text}"
        
        return result
    
    def summarize_text(self, text: str) -> str:
        """
        G√©n√®re un r√©sum√© du texte
        
        Args:
            text: Texte √† r√©sumer
            
        Returns:
            R√©sum√©
        """
        if not text.strip():
            return ""
        
        prompt = self.config.SUMMARY_PROMPT.format(text=text)
        result = self.generate_response(prompt, max_tokens=200, temperature=0.5)
        
        if result is None:
            # Fallback: cr√©er un r√©sum√© basique
            sentences = text.split('.')
            summary = '. '.join(sentences[:3])
            return f"‚ö†Ô∏è R√©sum√© automatique (basique):\n\n{summary}...\n\nüí° Pour un meilleur r√©sum√©, configurez une cl√© API OpenAI dans le fichier .env"
        
        return result
    
    def answer_question(self, text: str, question: str) -> str:
        """
        R√©pond √† une question bas√©e sur le texte extrait
        
        Args:
            text: Texte de r√©f√©rence
            question: Question de l'utilisateur
            
        Returns:
            R√©ponse
        """
        if not text.strip() or not question.strip():
            return ""
        
        prompt = self.config.QA_PROMPT.format(text=text, question=question)
        result = self.generate_response(prompt, max_tokens=300, temperature=0.4)
        
        if result is None:
            # Fallback: recherche simple de mots-cl√©s
            question_lower = question.lower()
            text_lower = text.lower()
            
            # Trouver la phrase la plus pertinente
            sentences = text.split('.')
            relevant_sentences = []
            
            for sentence in sentences:
                # Compter les mots de la question pr√©sents dans la phrase
                words = question_lower.split()
                matches = sum(1 for word in words if len(word) > 3 and word in sentence.lower())
                if matches > 0:
                    relevant_sentences.append((sentence, matches))
            
            if relevant_sentences:
                # Trier par pertinence
                relevant_sentences.sort(key=lambda x: x[1], reverse=True)
                best_sentence = relevant_sentences[0][0]
                return f"‚ö†Ô∏è R√©ponse basique (recherche de mots-cl√©s):\n\n{best_sentence}\n\nüí° Pour des r√©ponses plus intelligentes, configurez une cl√© API OpenAI dans le fichier .env"
            else:
                return "‚ö†Ô∏è Aucune r√©ponse trouv√©e dans le texte.\n\nüí° Pour des r√©ponses plus intelligentes, configurez une cl√© API OpenAI dans le fichier .env"
        
        return result
    
    def detect_language(self, text: str) -> str:
        """
        D√©tecte la langue du texte
        
        Args:
            text: Texte √† analyser
            
        Returns:
            Code de langue d√©tect√©
        """
        if not text.strip():
            return "unknown"
        
        # Essayer avec le mod√®le AI d'abord
        prompt = f"""
        D√©tectez la langue du texte suivant et r√©pondez uniquement par le code de langue (fr, en, es, de, it, pt, ru, zh, ar, ja, ko):
        
        Texte: {text[:200]}...
        
        Langue:
        """
        
        response = self.generate_response(prompt, max_tokens=10, temperature=0.1)
        
        if response is None:
            # Fallback: d√©tection basique par mots-cl√©s
            try:
                from langdetect import detect
                detected = detect(text)
                # Mapper les codes ISO vers nos codes
                lang_map = {
                    'fr': 'fr', 'en': 'en', 'es': 'es', 'de': 'de', 
                    'it': 'it', 'pt': 'pt', 'ru': 'ru', 'zh-cn': 'zh',
                    'zh-tw': 'zh', 'ar': 'ar', 'ja': 'ja', 'ko': 'ko'
                }
                return lang_map.get(detected, 'fr')
            except:
                # D√©tection ultra-basique par caract√®res
                if any('\u4e00' <= c <= '\u9fff' for c in text):
                    return 'zh'
                elif any('\u3040' <= c <= '\u309f' or '\u30a0' <= c <= '\u30ff' for c in text):
                    return 'ja'
                elif any('\uac00' <= c <= '\ud7af' for c in text):
                    return 'ko'
                elif any('\u0600' <= c <= '\u06ff' for c in text):
                    return 'ar'
                elif any('\u0400' <= c <= '\u04ff' for c in text):
                    return 'ru'
                else:
                    return 'fr'  # D√©faut
        
        # Nettoyer la r√©ponse
        detected_lang = response.strip().lower()
        
        # V√©rifier si c'est un code de langue valide
        valid_langs = list(self.config.TRANSLATION_LANGUAGES.keys())
        if detected_lang in valid_langs:
            return detected_lang
        
        # Essayer de mapper des r√©ponses communes
        lang_mapping = {
            'french': 'fr', 'fran√ßais': 'fr',
            'english': 'en', 'anglais': 'en',
            'spanish': 'es', 'espagnol': 'es',
            'german': 'de', 'allemand': 'de',
            'italian': 'it', 'italien': 'it',
            'portuguese': 'pt', 'portugais': 'pt',
            'russian': 'ru', 'russe': 'ru',
            'chinese': 'zh', 'chinois': 'zh',
            'arabic': 'ar', 'arabe': 'ar',
            'japanese': 'ja', 'japonais': 'ja',
            'korean': 'ko', 'cor√©en': 'ko'
        }
        
        for key, value in lang_mapping.items():
            if key in detected_lang:
                return value
        
        return "fr"  # D√©faut fran√ßais
    
    def enhance_ocr_text(self, text: str, confidence: float) -> str:
        """
        Am√©liore le texte OCR en corrigeant les erreurs potentielles
        
        Args:
            text: Texte OCR brut
            confidence: Niveau de confiance OCR
            
        Returns:
            Texte am√©lior√©
        """
        if not text.strip() or confidence > 0.9:
            return text
        
        prompt = f"""
        Le texte suivant a √©t√© extrait par OCR avec un niveau de confiance de {confidence:.2f}. 
        Corrigez les erreurs potentielles tout en pr√©servant le sens original:
        
        Texte OCR: {text}
        
        Texte corrig√©:
        """
        
        return self.generate_response(prompt, max_tokens=len(text) + 100, temperature=0.2)